{
    "model_name_or_path": "segment-any-text/sat-3l-sm",
    "output_dir": "sat-3l_lora_DUMMY-EXAMPLE",
    "text_path": "/content/wtpsplit/wtpsplit/utils/mon-corpus.pth",
    "block_size": 256,
    "overflow_size": 8,
    "eval_stride": 128,
    "dataloader_pin_memory": true,
    "collate_chunk_size": 512,
    "do_train": true,
    "do_eval": true,
    "per_device_train_batch_size": 256,
    "per_device_eval_batch_size": 256,
    "gradient_accumulation_steps": 8,
    "max_steps": 50,
    "load_best_model_at_end": false,
    "eval_accumulation_steps": 1,
    "dataloader_num_workers": 0,
    "preprocessing_num_workers": 1,
    "learning_rate": 3e-4,
    "bf16": true,
    "fp16": false,
    "optim": "adamw_torch_fused",
    "num_train_epochs": 10,
    "logging_steps": 10,
    "evaluation_strategy": "epoch",
    "save_strategy": "epoch",
    "report_to": "none",
    "wandb_project": "sentence",
    "save_steps": 100000000,
    "remove_unused_columns": false,
    "one_sample_per_line": false,
    "do_sentence_training": true,
    "do_auxiliary_training": false,
    "warmup_ratio": 0.1,
    "non_punctuation_sample_ratio": null,
    "prediction_loss_only": true,
    "use_auxiliary": true,
    "ddp_timeout": 3600,
    "use_subwords": true,
    "custom_punctuation_file": "punctuation_xlmr_unk.txt",
    "log_level": "warning",
    "adapter_config": "lora[r=16,alpha=32,intermediate_lora=True]",
    "weight_decay": 0.0,
    "auxiliary_remove_prob": 0.0,
    "train_adapter": true,
    "subsample": 10000
}
