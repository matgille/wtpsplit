{
  "model_name_or_path": "segment-any-text/sat-12l",
  "output_dir": "sat-12l_lora_DUMMY-EXAMPLE",
  "text_path": "/content/wtpsplit/wtpsplit/utils/mon-corpus.pth",
  "block_size": 384,
  "overflow_size": 32,
  "use_loss_weights": true,
  "eval_stride": 128,
  "dataloader_pin_memory": true,
  "collate_chunk_size": 512,
  "do_train": true,
  "do_eval": true,
  "per_device_train_batch_size": 256,
  "per_device_eval_batch_size": 256,
  "gradient_accumulation_steps": 8,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.2,
  "weight_decay": 0.01,
  "max_grad_norm": 1.0,
  "eval_accumulation_steps": 1,
  "dataloader_num_workers": 0,
  "preprocessing_num_workers": 1,
  "learning_rate": 1e-4,
  "bf16": true,
  "fp16": false,
  "optim": "adamw_torch_fused",
  "num_train_epochs": 50,
  "logging_steps": 10,
  "report_to": "none",
  "wandb_project": "sentence",
  "save_steps": 100000000,
  "remove_unused_columns": false,
  "one_sample_per_line": false,
  "do_sentence_training": true,
  "do_auxiliary_training": false,
  "non_punctuation_sample_ratio": null,
  "prediction_loss_only": true,
  "use_auxiliary": true,
  "ddp_timeout": 3600,
  "use_subwords": true,
  "custom_punctuation_file": "punctuation_xlmr_unk.txt",
  "log_level": "warning",
  "adapter_config": "lora[r=16,alpha=32,intermediate_lora=True]",
  "auxiliary_remove_prob": 0.0,
  "train_adapter": true,

  "early_stop_patience": 10,
  "early_stop_min_delta": 1e-4,
  "early_stop_monitor": "f1_best"
}
